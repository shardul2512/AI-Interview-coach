{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shardul2512/AI-Interview-coach/blob/main/malicious%20url%20detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lj7dEb5wR81"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier  # We can replace this with our best model\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sid321axn/malicious-urls-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "# If you haven't trained a model yet, this will train one automatically\n",
        "def train_model_if_needed():\n",
        "    if not os.path.exists('best_malicious_url_model.pkl') or not os.path.exists('url_vectorizer.pkl') or not os.path.exists('url_scaler.pkl'):\n",
        "        print(\"No trained model found. Training a new model...\")\n",
        "        # Load sample dataset - replace with your actual dataset file\n",
        "        try:\n",
        "            df = pd.read_csv('malicious_urls.csv')\n",
        "        except FileNotFoundError:\n",
        "            print(\"Sample dataset not found. Creating a minimal dataset for demonstration.\")\n",
        "            # Create a minimal sample dataset for demonstration\n",
        "            urls = [\n",
        "                'google.com', 'facebook.com', 'twitter.com', 'microsoft.com',  # Safe examples\n",
        "                'free-v-bucks.com', 'get-free-bitcoins.net', 'login-secure-paypal.com', 'verify-account-apple.com'  # Malicious examples\n",
        "            ]\n",
        "            labels = [0, 0, 0, 0, 1, 1, 1, 1]  # 0 for safe, 1 for malicious\n",
        "            df = pd.DataFrame({'url': urls, 'label': labels})\n",
        "            df.to_csv('malicious_urls.csv', index=False)\n",
        "\n",
        "        # Feature extraction\n",
        "        X, y, vectorizer, scaler = extract_features_for_training(df)\n",
        "\n",
        "        # Train a simple Random Forest model\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        model.fit(X, y)\n",
        "\n",
        "        # Save the model and preprocessing components\n",
        "        with open('best_malicious_url_model.pkl', 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        with open('url_vectorizer.pkl', 'wb') as f:\n",
        "            pickle.dump(vectorizer, f)\n",
        "        with open('url_scaler.pkl', 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "\n",
        "        print(\"Model trained and saved successfully.\")\n",
        "    else:\n",
        "        print(\"Trained model found. Ready to analyze URLs.\")\n",
        "\n",
        "# Extract features for training\n",
        "def extract_features_for_training(df):\n",
        "    # TF-IDF on raw URLs\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    url_features = vectorizer.fit_transform(df['url'])\n",
        "\n",
        "    # Engineer additional features\n",
        "    additional_features = engineer_url_features(df)\n",
        "\n",
        "    # Scale numerical features\n",
        "    scaler = StandardScaler()\n",
        "    additional_features_scaled = scaler.fit_transform(additional_features)\n",
        "\n",
        "    # Combine features\n",
        "    X = np.hstack((url_features.toarray(), additional_features_scaled))\n",
        "    y = df['label'].values\n",
        "\n",
        "    return X, y, vectorizer, scaler\n",
        "\n",
        "# Engineer features for a DataFrame of URLs\n",
        "def engineer_url_features(df):\n",
        "    df['url_length'] = df['url'].apply(len)\n",
        "    df['num_dots'] = df['url'].apply(lambda x: x.count('.'))\n",
        "    df['num_digits'] = df['url'].apply(lambda x: sum(c.isdigit() for c in x))\n",
        "    df['num_special_chars'] = df['url'].apply(lambda x: sum(not c.isalnum() and not c.isspace() for c in x))\n",
        "    df['has_https'] = df['url'].apply(lambda x: 1 if 'https://' in x else 0)\n",
        "    df['has_http'] = df['url'].apply(lambda x: 1 if 'http://' in x and 'https://' not in x else 0)\n",
        "    df['has_www'] = df['url'].apply(lambda x: 1 if 'www.' in x else 0)\n",
        "    df['num_hyphens'] = df['url'].apply(lambda x: x.count('-'))\n",
        "    df['num_underscores'] = df['url'].apply(lambda x: x.count('_'))\n",
        "    df['num_query_params'] = df['url'].apply(count_query_params)\n",
        "    df['domain_length'] = df['url'].apply(lambda x: len(extract_domain(x)))\n",
        "\n",
        "    return df[['url_length', 'num_dots', 'num_digits', 'num_special_chars',\n",
        "              'has_https', 'has_http', 'has_www', 'num_hyphens', 'num_underscores',\n",
        "              'num_query_params', 'domain_length']].values\n",
        "\n",
        "# Count query parameters in URL\n",
        "def count_query_params(url):\n",
        "    try:\n",
        "        query = urlparse(url).query if urlparse(url).scheme else urlparse('http://' + url).query\n",
        "        return len(query.split('&')) if query else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Extract domain from URL\n",
        "def extract_domain(url):\n",
        "    try:\n",
        "        parsed = urlparse(url).netloc if urlparse(url).scheme else urlparse('http://' + url).netloc\n",
        "        return parsed\n",
        "    except:\n",
        "        return url\n",
        "\n",
        "# Extract features for a single URL\n",
        "def extract_features_for_url(url, vectorizer, scaler):\n",
        "    # Create a DataFrame with a single URL\n",
        "    df = pd.DataFrame({'url': [url]})\n",
        "\n",
        "    # TF-IDF features\n",
        "    url_features = vectorizer.transform([url])\n",
        "\n",
        "    # Additional features\n",
        "    additional_features = engineer_url_features(df)\n",
        "    additional_features_scaled = scaler.transform(additional_features)\n",
        "\n",
        "    # Combine features\n",
        "    X = np.hstack((url_features.toarray(), additional_features_scaled))\n",
        "\n",
        "    return X\n",
        "\n",
        "# Normalize URL for analysis\n",
        "def normalize_url(url):\n",
        "    # Add http:// if no protocol specified\n",
        "    if not re.match(r'^https?://', url):\n",
        "        url = 'http://' + url\n",
        "    # Remove trailing slash\n",
        "    url = url.rstrip('/')\n",
        "    return url\n",
        "\n",
        "# Analyze URL and return result\n",
        "def analyze_url(url):\n",
        "    # Normalize the URL\n",
        "    url = normalize_url(url)\n",
        "\n",
        "    # Load the trained model and preprocessing components\n",
        "    with open('best_malicious_url_model.pkl', 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    with open('url_vectorizer.pkl', 'rb') as f:\n",
        "        vectorizer = pickle.load(f)\n",
        "    with open('url_scaler.pkl', 'rb') as f:\n",
        "        scaler = pickle.load(f)\n",
        "\n",
        "    # Extract features\n",
        "    features = extract_features_for_url(url, vectorizer, scaler)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(features)[0]\n",
        "    probability = model.predict_proba(features)[0][1]\n",
        "\n",
        "    # Determine risk level\n",
        "    if probability < 0.3:\n",
        "        risk_level = \"Low Risk\"\n",
        "    elif probability < 0.7:\n",
        "        risk_level = \"Medium Risk\"\n",
        "    else:\n",
        "        risk_level = \"High Risk\"\n",
        "\n",
        "    # Return result\n",
        "    result = {\n",
        "        'url': url,\n",
        "        'is_malicious': bool(prediction),\n",
        "        'probability': float(probability),\n",
        "        'risk_level': risk_level\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Simple command-line interface\n",
        "def main():\n",
        "    print(\"=== Malicious URL Detector ===\")\n",
        "    print(\"This tool analyzes URLs to detect potentially malicious websites.\")\n",
        "\n",
        "    # Check if we need to train a model first\n",
        "    train_model_if_needed()\n",
        "\n",
        "    while True:\n",
        "        # Get URL input\n",
        "        url = input(\"\\nEnter a URL to analyze (or 'exit' to quit): \")\n",
        "\n",
        "        if url.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Analyze the URL\n",
        "        try:\n",
        "            result = analyze_url(url)\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\nAnalysis Results:\")\n",
        "            print(f\"URL: {result['url']}\")\n",
        "            print(f\"Classification: {'MALICIOUS' if result['is_malicious'] else 'BENIGN'}\")\n",
        "            print(f\"Confidence: {result['probability']*100:.2f}%\")\n",
        "            print(f\"Risk Level: {result['risk_level']}\")\n",
        "\n",
        "            # Additional security advice\n",
        "            if result['is_malicious']:\n",
        "                print(\"\\nWARNING: This URL appears to be malicious. Do not visit this website.\")\n",
        "                print(\"It may be used for phishing, malware distribution, or other malicious activities.\")\n",
        "            else:\n",
        "                print(\"\\nThis URL appears to be benign, but always exercise caution when visiting unknown websites.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing URL: {str(e)}\")\n",
        "\n",
        "    print(\"Thank you for using the Malicious URL Detector.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUDDoQWjg2smrEtHhAJNsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}